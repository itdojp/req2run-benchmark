#!/usr/bin/env python3
"""
Universal Framework Adapter Interface (UFAI) CLI
Bench tool for running standardized benchmarks across frameworks
"""

import os
import sys
import json
import yaml
import time
import subprocess
import argparse
import shlex
from pathlib import Path
from typing import Dict, Any, Optional, List
import jsonschema

__version__ = "1.0.0"

class BenchmarkRunner:
    """Main benchmark runner class."""
    
    def __init__(self, config_path: str = "bench.yaml"):
        self.config_path = config_path
        self.config = self._load_config()
        self.results = {
            "version": "1.0",
            "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
            "framework": self.config.get("framework", {}),
            "compliance": self.config.get("compliance", {}),
            "stages": {},
            "overall": {}
        }
    
    def _load_config(self) -> Dict[str, Any]:
        """Load and validate bench.yaml configuration."""
        if not os.path.exists(self.config_path):
            raise FileNotFoundError(f"Configuration file not found: {self.config_path}")
        
        with open(self.config_path, 'r') as f:
            config = yaml.safe_load(f)
        
        # Validate against schema if available
        # Allow schema path to be set via env var, or search common locations
        schema_env = os.environ.get("BENCH_SCHEMA_PATH")
        candidate_paths = []
        if schema_env:
            candidate_paths.append(schema_env)
        # Look in current directory
        candidate_paths.append(str(Path.cwd() / "bench-schema.json"))
        # Look in same directory as config file
        config_dir = str(Path(self.config_path).parent)
        candidate_paths.append(str(Path(config_dir) / "bench-schema.json"))
        # Look in docs/ufai directory relative to bench script
        bench_dir = Path(__file__).parent.parent.parent
        candidate_paths.append(str(bench_dir / "docs" / "ufai" / "bench-schema.json"))
        # Fallback to /etc
        candidate_paths.append("/etc/bench/bench-schema.json")
        
        schema_path = None
        for path in candidate_paths:
            if os.path.exists(path):
                schema_path = path
                break
        
        if schema_path:
            with open(schema_path, 'r') as f:
                schema = json.load(f)
            try:
                jsonschema.validate(config, schema)
            except jsonschema.ValidationError as e:
                print(f"Configuration validation error: {e}")
                sys.exit(1)
        
        return config
    
    def _validate_command(self, command: str) -> bool:
        """Validate command for basic security checks."""
        # Dangerous patterns to check for
        dangerous_patterns = [
            "rm -rf /",  # Dangerous deletion
            ":(){ :|:& };:",  # Fork bomb
            "> /dev/sda",  # Disk overwrite
            "dd if=/dev/zero",  # Disk operations
        ]
        
        for pattern in dangerous_patterns:
            if pattern in command:
                print(f"Security warning: Dangerous pattern detected in command")
                return False
        
        return True
    
    def run_command(self, cmd_config: Any, stage: str) -> Dict[str, Any]:
        """Run a benchmark command."""
        if isinstance(cmd_config, str):
            command = cmd_config
            timeout = 300
        else:
            command = cmd_config.get("script", "")
            timeout = cmd_config.get("timeout", 300)
        
        # Validate command for security
        if not self._validate_command(command):
            return {
                "status": "error",
                "duration": 0,
                "error": "Command failed security validation"
            }
        
        print(f"Running {stage}: {command}")
        start_time = time.time()
        
        try:
            # Note: We use shell=True because bench.yaml commands often contain
            # shell features like pipes, redirections, and variable expansions.
            # Security is handled through validation and sandboxing (Docker).
            result = subprocess.run(
                command,
                shell=True,
                capture_output=True,
                text=True,
                timeout=timeout
            )
            
            duration = time.time() - start_time
            
            return {
                "status": "success" if result.returncode == 0 else "failed",
                "duration": round(duration, 2),
                "exit_code": result.returncode,
                "stdout": result.stdout,
                "stderr": result.stderr
            }
        
        except subprocess.TimeoutExpired:
            return {
                "status": "timeout",
                "duration": timeout,
                "error": f"Command timed out after {timeout} seconds"
            }
        
        except Exception as e:
            return {
                "status": "error",
                "duration": time.time() - start_time,
                "error": str(e)
            }
    
    def run_build(self) -> bool:
        """Run build stage."""
        if "build" not in self.config.get("commands", {}):
            print("No build command defined")
            return False
        
        result = self.run_command(self.config["commands"]["build"], "build")
        self.results["stages"]["build"] = result
        
        return result["status"] == "success"
    
    def run_test(self) -> bool:
        """Run test stage."""
        if "test" not in self.config.get("commands", {}):
            print("No test command defined")
            return False
        
        result = self.run_command(self.config["commands"]["test"], "test")
        
        # Parse test results if possible
        if result["status"] == "success":
            # Try to extract metrics from output
            result["metrics"] = self._parse_test_metrics(result.get("stdout", ""))
        
        self.results["stages"]["test"] = result
        return result["status"] == "success"
    
    def run_performance(self) -> bool:
        """Run performance stage."""
        if "performance" not in self.config.get("commands", {}):
            print("No performance command defined")
            return False
        
        perf_config = self.config["commands"]["performance"]
        
        # Start application if endpoint is specified
        endpoint = perf_config.get("endpoint") if isinstance(perf_config, dict) else None
        
        result = self.run_command(perf_config, "performance")
        
        # Parse performance results
        if result["status"] == "success":
            result["metrics"] = self._parse_performance_metrics(result.get("stdout", ""))
        
        self.results["stages"]["performance"] = result
        return result["status"] == "success"
    
    def run_security(self) -> bool:
        """Run security stage."""
        if "security" not in self.config.get("commands", {}):
            print("No security command defined")
            return False
        
        result = self.run_command(self.config["commands"]["security"], "security")
        
        # Parse security results
        if result["status"] == "success":
            result["metrics"] = self._parse_security_metrics(result.get("stdout", ""))
        
        self.results["stages"]["security"] = result
        return result["status"] == "success"
    
    def run_all(self) -> bool:
        """Run all stages."""
        stages = [
            ("build", self.run_build),
            ("test", self.run_test),
            ("performance", self.run_performance),
            ("security", self.run_security)
        ]
        
        success_count = 0
        total_count = 0
        
        for stage_name, stage_func in stages:
            if stage_name in self.config.get("commands", {}):
                total_count += 1
                if stage_func():
                    success_count += 1
                    print(f"✓ {stage_name} completed successfully")
                else:
                    print(f"✗ {stage_name} failed")
        
        # Calculate overall score
        self.results["overall"]["score"] = round((success_count / total_count) * 100, 2) if total_count > 0 else 0
        self.results["overall"]["grade"] = self._calculate_grade(self.results["overall"]["score"])
        
        return success_count == total_count
    
    def _parse_test_metrics(self, output: str) -> Dict[str, Any]:
        """Parse test metrics from output."""
        metrics = {}
        
        # Common patterns for test frameworks
        import re
        
        # pytest style
        if "passed" in output:
            match = re.search(r"(\d+) passed", output)
            if match:
                metrics["passed"] = int(match.group(1))
        
        # Coverage
        if "coverage" in output.lower():
            match = re.search(r"(\d+)%", output)
            if match:
                metrics["coverage"] = int(match.group(1))
        
        return metrics
    
    def _parse_performance_metrics(self, output: str) -> Dict[str, Any]:
        """Parse performance metrics from output."""
        metrics = {}
        
        # Parse common performance metrics
        import re
        
        # Requests per second
        if "req/s" in output or "requests/sec" in output:
            match = re.search(r"([\d.]+)\s*(?:req/s|requests/sec)", output)
            if match:
                metrics["requests_per_second"] = float(match.group(1))
        
        # Latency
        if "latency" in output.lower():
            match = re.search(r"p50[:\s]+([\d.]+)", output)
            if match:
                metrics["latency_p50"] = float(match.group(1))
        
        return metrics
    
    def _parse_security_metrics(self, output: str) -> Dict[str, Any]:
        """Parse security metrics from output."""
        metrics = {"vulnerabilities": {}}
        
        # Parse vulnerability counts
        import re
        
        for severity in ["critical", "high", "medium", "low"]:
            match = re.search(f"{severity}[:\s]+(\d+)", output, re.IGNORECASE)
            if match:
                metrics["vulnerabilities"][severity] = int(match.group(1))
        
        return metrics
    
    def _calculate_grade(self, score: float) -> str:
        """Calculate letter grade from score."""
        if score >= 95:
            return "A+"
        elif score >= 90:
            return "A"
        elif score >= 85:
            return "B+"
        elif score >= 80:
            return "B"
        elif score >= 75:
            return "C+"
        elif score >= 70:
            return "C"
        elif score >= 60:
            return "D"
        else:
            return "F"
    
    def save_results(self, output_path: str = "results.json"):
        """Save results to JSON file."""
        with open(output_path, 'w') as f:
            json.dump(self.results, f, indent=2)
        print(f"Results saved to {output_path}")
    
    def generate_report(self, format: str = "text"):
        """Generate human-readable report."""
        if format == "text":
            print("\n" + "=" * 60)
            print(f"Benchmark Results for {self.config['framework']['name']}")
            print("=" * 60)
            
            for stage, result in self.results["stages"].items():
                status_icon = "✓" if result["status"] == "success" else "✗"
                print(f"\n{status_icon} {stage.upper()}")
                print(f"  Status: {result['status']}")
                print(f"  Duration: {result.get('duration', 0)}s")
                
                if "metrics" in result:
                    print("  Metrics:")
                    for key, value in result["metrics"].items():
                        print(f"    {key}: {value}")
            
            print("\n" + "=" * 60)
            print(f"Overall Score: {self.results['overall']['score']}%")
            print(f"Grade: {self.results['overall']['grade']}")
            print("=" * 60)

def main():
    parser = argparse.ArgumentParser(
        description="Universal Framework Adapter Interface (UFAI) Benchmark Tool"
    )
    
    parser.add_argument(
        "command",
        choices=["init", "validate", "build", "test", "perf", "performance", 
                 "security", "all", "report", "export", "version"],
        help="Command to execute"
    )
    
    parser.add_argument(
        "-c", "--config",
        default="bench.yaml",
        help="Path to bench.yaml configuration file"
    )
    
    parser.add_argument(
        "-o", "--output",
        default="results.json",
        help="Output file for results"
    )
    
    parser.add_argument(
        "--format",
        choices=["json", "text", "html"],
        default="text",
        help="Output format for reports"
    )
    
    args = parser.parse_args()
    
    if args.command == "version":
        print(f"bench version {__version__}")
        sys.exit(0)
    
    if args.command == "init":
        # Create a sample bench.yaml
        sample_config = {
            "version": "1.0",
            "framework": {
                "name": "MyFramework",
                "language": "python",
                "version": "1.0.0"
            },
            "commands": {
                "build": "echo 'Building...'",
                "test": "echo 'Testing...'"
            }
        }
        
        with open("bench.yaml", 'w') as f:
            yaml.dump(sample_config, f, default_flow_style=False)
        
        print("Created bench.yaml")
        sys.exit(0)
    
    # Load configuration and run commands
    try:
        runner = BenchmarkRunner(args.config)
        
        if args.command == "validate":
            print("Configuration is valid")
        
        elif args.command == "build":
            success = runner.run_build()
            runner.save_results(args.output)
            sys.exit(0 if success else 1)
        
        elif args.command == "test":
            success = runner.run_test()
            runner.save_results(args.output)
            sys.exit(0 if success else 1)
        
        elif args.command in ["perf", "performance"]:
            success = runner.run_performance()
            runner.save_results(args.output)
            sys.exit(0 if success else 1)
        
        elif args.command == "security":
            success = runner.run_security()
            runner.save_results(args.output)
            sys.exit(0 if success else 1)
        
        elif args.command == "all":
            success = runner.run_all()
            runner.save_results(args.output)
            runner.generate_report(args.format)
            sys.exit(0 if success else 1)
        
        elif args.command == "report":
            runner.generate_report(args.format)
        
        elif args.command == "export":
            runner.save_results(args.output)
    
    except Exception as e:
        print(f"Error: {e}", file=sys.stderr)
        sys.exit(1)

if __name__ == "__main__":
    main()