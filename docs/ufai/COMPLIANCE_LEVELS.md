# UFAI Compliance Levels Specification
# UFAI„Ç≥„É≥„Éó„É©„Ç§„Ç¢„É≥„Çπ„É¨„Éô„É´‰ªïÊßò

## Overview / Ê¶ÇË¶Å

The UFAI Compliance Levels provide a progressive framework for frameworks to achieve benchmarking capabilities. Each level builds upon the previous one, allowing frameworks to start simple and gradually add more sophisticated features.

UFAI„Ç≥„É≥„Éó„É©„Ç§„Ç¢„É≥„Çπ„É¨„Éô„É´„ÅØ„ÄÅ„Éï„É¨„Éº„É†„ÉØ„Éº„ÇØ„Åå„Éô„É≥„ÉÅ„Éû„Éº„ÇØÊ©üËÉΩ„ÇíÊÆµÈöéÁöÑ„Å´ÈÅîÊàê„Åô„Çã„Åü„ÇÅ„ÅÆ„Éï„É¨„Éº„É†„ÉØ„Éº„ÇØ„ÇíÊèê‰æõ„Åó„Åæ„Åô„ÄÇÂêÑ„É¨„Éô„É´„ÅØÂâç„ÅÆ„É¨„Éô„É´„ÅÆ‰∏ä„Å´ÊßãÁØâ„Åï„Çå„ÄÅ„Éï„É¨„Éº„É†„ÉØ„Éº„ÇØ„ÅØ„Ç∑„É≥„Éó„É´„Å´Âßã„ÇÅ„Å¶Âæê„ÄÖ„Å´„Çà„ÇäÊ¥óÁ∑¥„Åï„Çå„ÅüÊ©üËÉΩ„ÇíËøΩÂä†„Åß„Åç„Åæ„Åô„ÄÇ

## Level Definitions / „É¨„Éô„É´ÂÆöÁæ©

### Level 0 (L0): Basic / Âü∫Êú¨
**Badge**: üîµ Blue  
**Requirements**:
- ‚úÖ Valid `bench.yaml` file exists
- ‚úÖ Build command executes successfully
- ‚úÖ Framework metadata is complete

**Validation Criteria**:
```yaml
# Minimum bench.yaml for L0
version: "1.0"
framework:
  name: "MyFramework"
  language: "python"
commands:
  build: "pip install -r requirements.txt"
```

**Benefits**:
- Listed in the framework registry
- Basic badge on leaderboard
- Eligible for automated testing

### Level 1 (L1): Testable / „ÉÜ„Çπ„ÉàÂèØËÉΩ
**Badge**: üü¢ Green  
**Requirements**:
- ‚úÖ All L0 requirements
- ‚úÖ Test command implemented
- ‚úÖ Tests execute and pass (‚â•80% pass rate)
- ‚úÖ Test output is parseable

**Validation Criteria**:
```yaml
commands:
  build: "..."
  test:
    script: "pytest tests/"
    timeout: 600
```

**Metrics Required**:
- Total tests count
- Passed/failed breakdown
- Execution time

### Level 2 (L2): Measured / Ê∏¨ÂÆöÊ∏à„Åø
**Badge**: üü° Yellow  
**Requirements**:
- ‚úÖ All L1 requirements
- ‚úÖ Performance testing implemented
- ‚úÖ Metrics collection configured
- ‚úÖ Resource usage tracked

**Validation Criteria**:
```yaml
commands:
  # ... previous commands
  performance:
    script: "locust -f perf_test.py"
    endpoint: "http://localhost:8000"
    duration: 60
    concurrency: 100
```

**Metrics Required**:
- Requests per second
- Latency percentiles (p50, p90, p95, p99)
- Error rate
- Resource usage (CPU, memory)

### Level 3 (L3): Secure / „Çª„Ç≠„É•„Ç¢
**Badge**: üü† Orange  
**Requirements**:
- ‚úÖ All L2 requirements
- ‚úÖ Security scanning implemented
- ‚úÖ Vulnerability reporting configured
- ‚úÖ Dependency checking enabled

**Validation Criteria**:
```yaml
commands:
  # ... previous commands
  security:
    script: "bandit -r src/"
    scan_type: "static"
    severity: "medium"
```

**Metrics Required**:
- Vulnerability count by severity
- Compliance score
- Security tool coverage

### Level 4 (L4): Observable / Ë¶≥Ê∏¨ÂèØËÉΩ
**Badge**: üî¥ Red  
**Requirements**:
- ‚úÖ All L3 requirements
- ‚úÖ Code coverage reporting (‚â•70%)
- ‚úÖ Logging standardized
- ‚úÖ Metrics exportable
- ‚úÖ Tracing implemented (optional)

**Validation Criteria**:
```yaml
commands:
  # ... previous commands
  coverage:
    script: "pytest --cov=src --cov-report=html"
    threshold: 70
artifacts:
  coverage: "htmlcov/index.html"
  logs: "logs/"
  metrics: "metrics.json"
```

**Metrics Required**:
- Line coverage percentage
- Branch coverage percentage
- Function coverage percentage
- Log format compliance

### Level 5 (L5): Complete / ÂÆåÂÖ®
**Badge**: üåü Gold Star  
**Requirements**:
- ‚úÖ All L4 requirements
- ‚úÖ All benchmark stages score ‚â•90%
- ‚úÖ Documentation complete
- ‚úÖ CI/CD integration demonstrated
- ‚úÖ Production readiness validated
- ‚úÖ Scalability testing passed

**Validation Criteria**:
```yaml
compliance:
  level: "L5"
  features:
    - build
    - test
    - performance
    - security
    - coverage
    - docs
    - deploy
    - scale
```

**Additional Requirements**:
- Comprehensive API documentation
- Deployment manifests (Docker, Kubernetes)
- Performance under load (10x baseline)
- Zero critical vulnerabilities
- Automated release process

## Compliance Matrix / „Ç≥„É≥„Éó„É©„Ç§„Ç¢„É≥„Çπ„Éû„Éà„É™„ÉÉ„ÇØ„Çπ

| Feature | L0 | L1 | L2 | L3 | L4 | L5 |
|---------|----|----|----|----|----|----|
| Build | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |
| Test | ‚ùå | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |
| Performance | ‚ùå | ‚ùå | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |
| Security | ‚ùå | ‚ùå | ‚ùå | ‚úÖ | ‚úÖ | ‚úÖ |
| Coverage | ‚ùå | ‚ùå | ‚ùå | ‚ùå | ‚úÖ | ‚úÖ |
| Documentation | ‚ùå | ‚ùå | ‚ùå | ‚ùå | ‚ùå | ‚úÖ |
| Production Ready | ‚ùå | ‚ùå | ‚ùå | ‚ùå | ‚ùå | ‚úÖ |

## Scoring Algorithm / „Çπ„Ç≥„Ç¢„É™„É≥„Ç∞„Ç¢„É´„Ç¥„É™„Ç∫„É†

### Base Score Calculation

```python
def calculate_compliance_score(results):
    weights = {
        'build': 0.10,      # L0
        'test': 0.20,       # L1
        'performance': 0.25, # L2
        'security': 0.20,   # L3
        'coverage': 0.15,   # L4
        'docs': 0.10        # L5
    }
    
    score = 0
    for stage, weight in weights.items():
        if stage in results['stages']:
            stage_score = calculate_stage_score(results['stages'][stage])
            score += stage_score * weight
    
    return score
```

### Level Determination

```python
def determine_compliance_level(results):
    score = results['overall']['score']
    features = results['compliance']['features']
    
    if score >= 90 and all_features_present(features):
        return 'L5'
    elif 'coverage' in features and score >= 80:
        return 'L4'
    elif 'security' in features and score >= 70:
        return 'L3'
    elif 'performance' in features and score >= 60:
        return 'L2'
    elif 'test' in features and score >= 50:
        return 'L1'
    elif 'build' in features:
        return 'L0'
    else:
        return None  # Not compliant
```

## Progressive Enhancement Guide / ÊÆµÈöéÁöÑÂº∑Âåñ„Ç¨„Ç§„Éâ

### Starting at L0

1. Create minimal `bench.yaml`:
```bash
bench init --level L0
```

2. Verify build works:
```bash
bench build
```

3. Submit for validation:
```bash
bench validate --submit
```

### Upgrading to L1

1. Add test configuration:
```yaml
commands:
  test:
    script: "npm test"
```

2. Ensure tests pass:
```bash
bench test
# Must achieve ‚â•80% pass rate
```

### Upgrading to L2

1. Add performance testing:
```yaml
commands:
  performance:
    script: "wrk -t12 -c400 -d30s http://localhost:8080/"
    endpoint: "http://localhost:8080"
```

2. Verify metrics collection:
```bash
bench perf
# Check results.json for metrics
```

### Upgrading to L3

1. Add security scanning:
```yaml
commands:
  security:
    script: "npm audit && snyk test"
    scan_type: "dependency"
```

2. Fix critical vulnerabilities:
```bash
bench security
# Address any critical/high findings
```

### Upgrading to L4

1. Add coverage reporting:
```yaml
commands:
  coverage:
    script: "jest --coverage"
artifacts:
  coverage: "coverage/lcov-report/index.html"
```

2. Achieve coverage threshold:
```bash
bench coverage
# Must achieve ‚â•70% coverage
```

### Achieving L5

1. Complete all features:
```yaml
compliance:
  level: "L5"
  features: [build, test, performance, security, coverage, docs, deploy]
```

2. Score ‚â•90% on all stages:
```bash
bench all --strict
# All stages must score ‚â•90%
```

## Compliance Badges / „Ç≥„É≥„Éó„É©„Ç§„Ç¢„É≥„Çπ„Éê„ÉÉ„Ç∏

### Markdown Badges

```markdown
![UFAI L0](https://bench.req2run.io/badge/L0)
![UFAI L1](https://bench.req2run.io/badge/L1)
![UFAI L2](https://bench.req2run.io/badge/L2)
![UFAI L3](https://bench.req2run.io/badge/L3)
![UFAI L4](https://bench.req2run.io/badge/L4)
![UFAI L5](https://bench.req2run.io/badge/L5)
```

### HTML Badges

```html
<img src="https://bench.req2run.io/badge/L5" alt="UFAI Level 5 Compliant">
```

### Dynamic Badge

```markdown
![UFAI Compliance](https://bench.req2run.io/badge/dynamic/myframework)
```

## Verification Process / Ê§úË®º„Éó„É≠„Çª„Çπ

### Automated Verification

1. **Daily Runs**: Frameworks are tested daily
2. **Version Tracking**: Each framework version is tested separately
3. **Historical Data**: Performance trends tracked over time
4. **Regression Detection**: Automatic alerts on score drops

### Manual Verification

For L4 and L5 compliance:
1. Code review of implementation
2. Production deployment verification
3. Scalability testing validation
4. Documentation quality check

## Benefits by Level / „É¨„Éô„É´Âà•„ÅÆ„É°„É™„ÉÉ„Éà

| Level | Benefits |
|-------|----------|
| **L0** | ‚Ä¢ Registry listing<br>‚Ä¢ Basic visibility<br>‚Ä¢ Community recognition |
| **L1** | ‚Ä¢ "Tested" badge<br>‚Ä¢ Quality assurance<br>‚Ä¢ CI/CD ready |
| **L2** | ‚Ä¢ Performance metrics<br>‚Ä¢ Benchmark comparisons<br>‚Ä¢ "Fast" badge eligibility |
| **L3** | ‚Ä¢ Security badge<br>‚Ä¢ Vulnerability tracking<br>‚Ä¢ Enterprise ready |
| **L4** | ‚Ä¢ "Production Ready" status<br>‚Ä¢ Observability proven<br>‚Ä¢ Premium listing |
| **L5** | ‚Ä¢ "Gold Standard" recognition<br>‚Ä¢ Featured framework<br>‚Ä¢ Reference implementation |

## FAQ / „Çà„Åè„ÅÇ„ÇãË≥™Âïè

### Q: Can I skip levels?
**A**: No, each level builds on the previous one. You must achieve L0 before L1, etc.

### Q: How often is compliance verified?
**A**: Daily for L0-L3, weekly for L4-L5.

### Q: What if my framework fails a level?
**A**: You retain your highest achieved level for 30 days to fix issues.

### Q: Can I use custom tools?
**A**: Yes, as long as they produce parseable output in expected formats.

### Q: Is L5 required for production use?
**A**: No, L3 is typically sufficient for production. L5 represents excellence.

## Support / „Çµ„Éù„Éº„Éà

- **Documentation**: https://bench.req2run.io/docs
- **Examples**: https://github.com/req2run/examples
- **Community**: https://discord.gg/req2run
- **Email**: support@req2run.io