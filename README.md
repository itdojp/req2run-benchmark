# Req2Run Benchmark

[English](#english) | [æ—¥æœ¬èª](#japanese)

---

<a id="english"></a>
## English

**A comprehensive benchmark framework for evaluating AI code generation systems from requirements to running code**

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Version](https://img.shields.io/badge/version-1.0.0-blue.svg)](https://github.com/itdojp/req2run-benchmark)
[![Status](https://img.shields.io/badge/status-active-green.svg)]()

### Overview

Req2Run (Requirements to Running Code) is a benchmark framework for quantitatively evaluating the implementation capabilities of AI/LLM systems. It generates working code from detailed requirement specifications and performs automated evaluation.

### Features

- ğŸ¯ **Comprehensive Problem Set**: 15+ categories including Web APIs, cryptography, network protocols, data processing
- ğŸ¤– **Fully Automated Evaluation**: Execution from generation to evaluation without human intervention
- ğŸ“Š **Quantitative Metrics**: Measures functional coverage, performance, security, and code quality
- ğŸ”§ **Standardized Environment**: Unified execution environment on Docker/Kubernetes
- ğŸ“ˆ **Difficulty Levels**: Four levels - Basic, Intermediate, Advanced, Expert

### Quick Start

#### Prerequisites

- Docker 24.0+
- Kubernetes 1.28+ (optional)
- Python 3.11+
- Git

#### Installation

```bash
# Clone the repository
git clone https://github.com/itdojp/req2run-benchmark.git
cd req2run-benchmark

# Install dependencies
pip install -r requirements.txt

# Setup environment
./scripts/setup/init.sh
```

#### Basic Usage

```bash
# Evaluate a single problem
python -m req2run evaluate --problem WEB-001 --submission ./submissions/my_solution

# Run batch evaluation
python -m req2run batch-evaluate --difficulty intermediate --output ./results

# Generate report
python -m req2run report --results ./results --format html
```

### Problem Categories

| Category | Description | Count |
|----------|-------------|-------|
| `web_api` | RESTful API implementation | 3 |
| `cli_tool` | CLI tool development | 2 |
| `network_protocol` | Custom protocols | 2 |
| `cryptography` | Encryption tools | 2 |
| `data_processing` | Data pipelines | 2 |
| `system_utility` | System utilities | 2 |
| `machine_learning` | ML pipelines | 1 |
| `database` | Database implementation | 2 |

### Evaluation Flow

```mermaid
graph LR
    A[Requirements] --> B[AI Generation]
    B --> C[Build]
    C --> D[Deploy]
    D --> E[Test Execution]
    E --> F[Metrics Collection]
    F --> G[Score Calculation]
    G --> H[Report]
```

### Directory Structure

```
req2run-benchmark/
â”œâ”€â”€ problems/           # Problem definitions (YAML)
â”‚   â”œâ”€â”€ basic/         # Basic level problems
â”‚   â”œâ”€â”€ intermediate/  # Intermediate level problems
â”‚   â”œâ”€â”€ advanced/      # Advanced level problems
â”‚   â””â”€â”€ expert/        # Expert level problems
â”œâ”€â”€ evaluation/        # Evaluation framework
â”‚   â”œâ”€â”€ harness/      # Test harness
â”‚   â”œâ”€â”€ metrics/      # Metrics calculation
â”‚   â””â”€â”€ reports/      # Report generation
â”œâ”€â”€ infrastructure/    # Infrastructure configs
â”‚   â”œâ”€â”€ docker/       # Docker containers
â”‚   â”œâ”€â”€ kubernetes/   # K8s manifests
â”‚   â””â”€â”€ terraform/    # IaC configurations
â”œâ”€â”€ scripts/          # Utility scripts
â”œâ”€â”€ tests/            # Test suite
â””â”€â”€ docs/             # Documentation
```

### Evaluation Criteria

Each problem is evaluated based on:

- **Functional Coverage** (30-40%)
- **Test Pass Rate** (20-30%)
- **Performance** (10-20%)
- **Code Quality** (10-20%)
- **Security** (10-20%)

Pass Criteria: Total Score â‰¥ 70%

### API Usage

```python
from req2run import Evaluator, Problem

# Load problem
problem = Problem.load("WEB-001")

# Initialize evaluator
evaluator = Evaluator(
    problem=problem,
    environment="docker",
    timeout=3600
)

# Run evaluation
result = evaluator.evaluate(
    submission_path="./my_solution",
    verbose=True
)

# Get results
print(f"Score: {result.score}")
print(f"Status: {result.status}")
```

### Contributing

1. Fork this repository
2. Create a feature branch (`git checkout -b feature/NewProblem`)
3. Commit your changes (`git commit -m 'Add new problem category'`)
4. Push to the branch (`git push origin feature/NewProblem`)
5. Create a Pull Request

See [CONTRIBUTING.md](docs/CONTRIBUTING.md) for details.

### License

This project is licensed under the MIT License. See [LICENSE](LICENSE) file for details.

### Contact

- Project Lead: ITDo Inc. Japan
- Email: contact@itdo.jp
- Issue Tracker: [GitHub Issues](https://github.com/itdojp/req2run-benchmark/issues)

### Acknowledgments

This project is made possible by contributions from the AI/LLM community.

---

<a id="japanese"></a>
## æ—¥æœ¬èª

**è¦æ±‚ä»•æ§˜ã‹ã‚‰å®Ÿå‹•ã‚³ãƒ¼ãƒ‰ã¾ã§AIã‚³ãƒ¼ãƒ‰ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ã‚’è©•ä¾¡ã™ã‚‹åŒ…æ‹¬çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯**

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Version](https://img.shields.io/badge/version-1.0.0-blue.svg)](https://github.com/itdojp/req2run-benchmark)
[![Status](https://img.shields.io/badge/status-active-green.svg)]()

### æ¦‚è¦

Req2Runï¼ˆRequirements to Running Codeï¼‰ã¯ã€AI/LLMã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…èƒ½åŠ›ã‚’å®šé‡çš„ã«è©•ä¾¡ã™ã‚‹ãŸã‚ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚è©³ç´°ãªè¦æ±‚ä»•æ§˜ã‹ã‚‰å®Ÿå‹•ã™ã‚‹ã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã—ã€è‡ªå‹•è©•ä¾¡ã‚’è¡Œã„ã¾ã™ã€‚

### ç‰¹å¾´

- ğŸ¯ **åŒ…æ‹¬çš„ãªå•é¡Œã‚»ãƒƒãƒˆ**: Web APIã€æš—å·åŒ–ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã€ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãªã©15ä»¥ä¸Šã®ã‚«ãƒ†ã‚´ãƒª
- ğŸ¤– **å®Œå…¨è‡ªå‹•è©•ä¾¡**: äººé–“ã®ä»‹å…¥ãªã—ã§ç”Ÿæˆã‹ã‚‰è©•ä¾¡ã¾ã§å®Ÿè¡Œ
- ğŸ“Š **å®šé‡çš„ãƒ¡ãƒˆãƒªã‚¯ã‚¹**: æ©Ÿèƒ½å……è¶³ç‡ã€æ€§èƒ½ã€ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã€ã‚³ãƒ¼ãƒ‰å“è³ªã‚’æ¸¬å®š
- ğŸ”§ **æ¨™æº–åŒ–ã•ã‚ŒãŸç’°å¢ƒ**: Docker/Kubernetesä¸Šã§ã®çµ±ä¸€å®Ÿè¡Œç’°å¢ƒ
- ğŸ“ˆ **é›£æ˜“åº¦ãƒ¬ãƒ™ãƒ«**: Basicã€Intermediateã€Advancedã€Expertã®4æ®µéš

### ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

#### å‰ææ¡ä»¶

- Docker 24.0+
- Kubernetes 1.28+ (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)
- Python 3.11+
- Git

#### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³
git clone https://github.com/itdojp/req2run-benchmark.git
cd req2run-benchmark

# ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -r requirements.txt

# ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
./scripts/setup/init.sh
```

#### åŸºæœ¬çš„ãªä½¿ç”¨æ–¹æ³•

```bash
# å˜ä¸€å•é¡Œã®è©•ä¾¡å®Ÿè¡Œ
python -m req2run evaluate --problem WEB-001 --submission ./submissions/my_solution

# ãƒãƒƒãƒè©•ä¾¡ã®å®Ÿè¡Œ
python -m req2run batch-evaluate --difficulty intermediate --output ./results

# ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
python -m req2run report --results ./results --format html
```

### å•é¡Œã‚«ãƒ†ã‚´ãƒª

| ã‚«ãƒ†ã‚´ãƒª | èª¬æ˜ | å•é¡Œæ•° |
|---------|------|--------|
| `web_api` | RESTful APIå®Ÿè£… | 3 |
| `cli_tool` | CLIãƒ„ãƒ¼ãƒ«é–‹ç™º | 2 |
| `network_protocol` | ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒˆã‚³ãƒ« | 2 |
| `cryptography` | æš—å·åŒ–ãƒ„ãƒ¼ãƒ« | 2 |
| `data_processing` | ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ | 2 |
| `system_utility` | ã‚·ã‚¹ãƒ†ãƒ ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ | 2 |
| `machine_learning` | ML ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ | 1 |
| `database` | ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å®Ÿè£… | 2 |

### è©•ä¾¡ãƒ•ãƒ­ãƒ¼

```mermaid
graph LR
    A[è¦æ±‚ä»•æ§˜] --> B[AIç”Ÿæˆ]
    B --> C[ãƒ“ãƒ«ãƒ‰]
    C --> D[ãƒ‡ãƒ—ãƒ­ã‚¤]
    D --> E[ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ]
    E --> F[ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†]
    F --> G[ã‚¹ã‚³ã‚¢ç®—å‡º]
    G --> H[ãƒ¬ãƒãƒ¼ãƒˆ]
```

### ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹æˆ

```
req2run-benchmark/
â”œâ”€â”€ problems/           # å•é¡Œå®šç¾©ï¼ˆYAMLï¼‰
â”‚   â”œâ”€â”€ basic/         # åŸºç¤ãƒ¬ãƒ™ãƒ«å•é¡Œ
â”‚   â”œâ”€â”€ intermediate/  # ä¸­ç´šãƒ¬ãƒ™ãƒ«å•é¡Œ
â”‚   â”œâ”€â”€ advanced/      # ä¸Šç´šãƒ¬ãƒ™ãƒ«å•é¡Œ
â”‚   â””â”€â”€ expert/        # ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ¬ãƒ™ãƒ«å•é¡Œ
â”œâ”€â”€ evaluation/        # è©•ä¾¡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
â”‚   â”œâ”€â”€ harness/      # ãƒ†ã‚¹ãƒˆãƒãƒ¼ãƒã‚¹
â”‚   â”œâ”€â”€ metrics/      # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
â”‚   â””â”€â”€ reports/      # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
â”œâ”€â”€ infrastructure/    # ã‚¤ãƒ³ãƒ•ãƒ©è¨­å®š
â”‚   â”œâ”€â”€ docker/       # Dockerã‚³ãƒ³ãƒ†ãƒŠ
â”‚   â”œâ”€â”€ kubernetes/   # K8sãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆ
â”‚   â””â”€â”€ terraform/    # IaCè¨­å®š
â”œâ”€â”€ scripts/          # ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”œâ”€â”€ tests/            # ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ
â””â”€â”€ docs/             # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
```

### è©•ä¾¡åŸºæº–

å„å•é¡Œã¯ä»¥ä¸‹ã®åŸºæº–ã§è©•ä¾¡ã•ã‚Œã¾ã™ï¼š

- **æ©Ÿèƒ½è¦ä»¶å……è¶³ç‡** (30-40%)
- **ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹é€šéç‡** (20-30%)
- **æ€§èƒ½è¦ä»¶é”æˆ** (10-20%)
- **ã‚³ãƒ¼ãƒ‰å“è³ª** (10-20%)
- **ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£** (10-20%)

åˆæ ¼åŸºæº–ï¼šç·åˆã‚¹ã‚³ã‚¢ â‰¥ 70%

### APIåˆ©ç”¨

```python
from req2run import Evaluator, Problem

# å•é¡Œã®ãƒ­ãƒ¼ãƒ‰
problem = Problem.load("WEB-001")

# è©•ä¾¡å™¨ã®åˆæœŸåŒ–
evaluator = Evaluator(
    problem=problem,
    environment="docker",
    timeout=3600
)

# è©•ä¾¡å®Ÿè¡Œ
result = evaluator.evaluate(
    submission_path="./my_solution",
    verbose=True
)

# çµæœã®å–å¾—
print(f"Score: {result.score}")
print(f"Status: {result.status}")
```

### è²¢çŒ®æ–¹æ³•

1. ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã‚’ãƒ•ã‚©ãƒ¼ã‚¯
2. ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ–ãƒ©ãƒ³ãƒã‚’ä½œæˆ (`git checkout -b feature/NewProblem`)
3. å¤‰æ›´ã‚’ã‚³ãƒŸãƒƒãƒˆ (`git commit -m 'Add new problem category'`)
4. ãƒ–ãƒ©ãƒ³ãƒã«ãƒ—ãƒƒã‚·ãƒ¥ (`git push origin feature/NewProblem`)
5. ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ä½œæˆ

è©³ç´°ã¯ [CONTRIBUTING.md](docs/CONTRIBUTING.md) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

### ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯MITãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã®ä¸‹ã§å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚è©³ç´°ã¯ [LICENSE](LICENSE) ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

### é€£çµ¡å…ˆ

- ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒªãƒ¼ãƒ€ãƒ¼: ITDo Inc. Japan
- Email: contact@itdo.jp
- Issue Tracker: [GitHub Issues](https://github.com/itdojp/req2run-benchmark/issues)

### è¬è¾

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ã€AI/LLMã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã®è²¢çŒ®ã«ã‚ˆã‚Šå®Ÿç¾ã—ã¦ã„ã¾ã™ã€‚

---

**Note**: ã“ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¯ç¶™ç¶šçš„ã«æ”¹å–„ã•ã‚Œã¦ã„ã¾ã™ã€‚æœ€æ–°æƒ…å ±ã¯ [ãƒªãƒªãƒ¼ã‚¹ãƒãƒ¼ãƒˆ](https://github.com/itdojp/req2run-benchmark/releases) ã‚’ã”ç¢ºèªãã ã•ã„ã€‚ | This benchmark is continuously being improved. Check [Release Notes](https://github.com/itdojp/req2run-benchmark/releases) for the latest updates.
