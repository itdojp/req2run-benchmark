# CLI-011: Parallel Job Orchestrator with DAG Execution

[English](#english) | [æ—¥æœ¬èª](#japanese)

---

<a id="english"></a>
## English

A sophisticated command-line tool for orchestrating parallel job execution with DAG (Directed Acyclic Graph) dependencies, featuring resource management, retry logic, and real-time monitoring.

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Workflow File  â”‚â”€â”€â”€â–¶â”‚   DAG Analyzer   â”‚â”€â”€â”€â–¶â”‚  Job Executor   â”‚
â”‚  (YAML/JSON)    â”‚    â”‚                  â”‚    â”‚                 â”‚
â”‚                 â”‚    â”‚ â€¢ Validation     â”‚    â”‚ â€¢ Resource Mgmt â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ â€¢ Cycle Detectionâ”‚    â”‚ â€¢ Retry Logic   â”‚
                       â”‚ â€¢ Level Planning â”‚    â”‚ â€¢ Process Mgmt  â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚                         â”‚
                               â–¼                         â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   Orchestrator   â”‚â—„â”€â”€â”€â”‚  Live Monitor   â”‚
                       â”‚                  â”‚    â”‚                 â”‚
                       â”‚ â€¢ Parallel Exec  â”‚    â”‚ â€¢ Real-time UI  â”‚
                       â”‚ â€¢ Status Trackingâ”‚    â”‚ â€¢ Progress Bars â”‚
                       â”‚ â€¢ Event Handling â”‚    â”‚ â€¢ Resource Statsâ”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Key Features

### ğŸ—ï¸ DAG-Based Execution
- **Dependency Resolution**: Automatic topological sorting of job dependencies
- **Cycle Detection**: Prevents circular dependencies with clear error messages
- **Parallel Execution**: Runs independent jobs concurrently up to resource limits
- **Critical Path Analysis**: Identifies longest execution path for optimization

### ğŸ”„ Advanced Job Management
- **Multiple Job Types**: Command, script, Python code, and HTTP requests
- **Retry Logic**: Exponential backoff with configurable max attempts
- **Conditional Execution**: Skip jobs based on dependency failures
- **Resource Limits**: CPU, memory, and execution time constraints

### ğŸ“Š Real-Time Monitoring
- **Live Progress Display**: Rich terminal UI with progress bars and status
- **Resource Tracking**: Monitor CPU, memory usage across all running jobs
- **Event History**: Complete audit trail of all job state changes
- **Status Callbacks**: Extensible event system for custom monitoring

### âš™ï¸ Resource Management
- **Concurrent Job Limits**: Configurable maximum parallel job execution
- **Memory Management**: Track and limit memory usage per job
- **Process Lifecycle**: Proper cleanup and signal handling
- **Graceful Shutdown**: Clean termination with running job cleanup

## Quick Start

### Prerequisites

- Python 3.11+
- Dependencies: `pip install -r requirements.txt`

### Installation

```bash
# Clone and setup
cd baselines/CLI-011
pip install -r requirements.txt

# Verify installation
python src/main.py --help
```

### Basic Usage

```bash
# Create a sample workflow
python src/main.py create --name "My Pipeline" --output workflow.yaml

# Analyze workflow structure
python src/main.py analyze workflow.yaml

# Execute workflow with live monitoring
python src/main.py run workflow.yaml --max-concurrent 4

# Dry run to validate without execution
python src/main.py run workflow.yaml --dry-run
```

### Docker Usage

```bash
# Build container
docker build -t job-orchestrator .

# Run with mounted workflow
docker run -v $(pwd)/workflows:/app/workflows job-orchestrator run /app/workflows/sample.yaml

# Interactive mode
docker run -it job-orchestrator bash
```

## Workflow Definition

### YAML Format

```yaml
name: "CI/CD Pipeline"
version: "1.0.0"
description: "Build and deployment pipeline"

global_config:
  max_retries: 3
  timeout: 3600

jobs:
  - id: "build"
    name: "Build Application"
    job_type: "command"
    command: "make build"
    dependencies: []
    timeout: 300
    retry_config:
      max_attempts: 3
      initial_delay: 1.0
      backoff_factor: 2.0
    resource_limits:
      max_memory_mb: 1024
      max_execution_time: 600
    environment:
      NODE_ENV: "production"
    tags: ["build", "compile"]

  - id: "test"
    name: "Run Tests"
    job_type: "script"
    script_path: "./scripts/test.sh"
    dependencies: ["build"]
    timeout: 600

  - id: "deploy"
    name: "Deploy Application"
    job_type: "command"
    command: "kubectl apply -f deployment.yaml"
    dependencies: ["test"]
    timeout: 900
```

### Job Types

#### Command Jobs
```yaml
job_type: "command"
command: "echo 'Hello World' && ls -la"
working_directory: "/app"
environment:
  PATH: "/usr/local/bin:$PATH"
```

#### Script Jobs
```yaml
job_type: "script"
script_path: "./scripts/deploy.sh"
working_directory: "/app"
```

#### Python Jobs
```yaml
job_type: "python"
command: |
  import os
  print(f"Current directory: {os.getcwd()}")
  # Your Python code here
```

#### HTTP Jobs
```yaml
job_type: "http"
command: "https://api.example.com/health"
timeout: 30
```

## CLI Commands

### `run` - Execute Workflow

```bash
python src/main.py run workflow.yaml [OPTIONS]

Options:
  -c, --max-concurrent INTEGER  Maximum concurrent jobs [default: 4]
  --dry-run                    Validate workflow without execution
  -o, --output PATH            Output results to JSON file
  --live / --no-live          Show live progress display [default: True]
```

### `analyze` - Analyze DAG Structure

```bash
python src/main.py analyze workflow.yaml [OPTIONS]

Options:
  -o, --output PATH  Output analysis to JSON file
```

Example output:
```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Property            â”ƒ Value               â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ Workflow Name       â”‚ Sample CI/CD        â”‚
â”‚ Total Jobs          â”‚ 14                  â”‚
â”‚ Valid DAG           â”‚ âœ“                   â”‚
â”‚ Has Cycles          â”‚ âœ“                   â”‚
â”‚ Execution Levels    â”‚ 8                   â”‚
â”‚ Critical Path       â”‚ 7                   â”‚
â”‚ Max Parallelism     â”‚ 3                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Execution Order:
  Level 0: setup
  Level 1: install_deps
  Level 2: lint, build_frontend, build_backend
  Level 3: test_unit_frontend, test_unit_backend, security_scan
  ...
```

### `create` - Generate Sample Workflow

```bash
python src/main.py create --name "Pipeline Name" --output workflow.yaml
```

## Configuration

### Retry Configuration

```yaml
retry_config:
  max_attempts: 3        # Maximum retry attempts
  initial_delay: 1.0     # Initial delay in seconds
  max_delay: 60.0        # Maximum delay between retries
  backoff_factor: 2.0    # Exponential backoff multiplier
```

### Resource Limits

```yaml
resource_limits:
  max_memory_mb: 1024         # Maximum memory per job
  max_cpu_percent: 100.0      # CPU usage limit
  max_execution_time: 3600    # Timeout in seconds
  max_concurrent_jobs: 4      # Global concurrency limit
```

### Environment Variables

```yaml
environment:
  NODE_ENV: "production"
  API_URL: "https://api.example.com"
  DEBUG: "false"
```

## Monitoring and Status

### Live Display

The orchestrator provides a rich terminal interface showing:

- **Real-time job status** with progress bars
- **Resource usage** (CPU, memory, active jobs)
- **Execution timeline** with start/completion times
- **Error messages** and retry attempts

### Status Callbacks

```python
def status_callback(event_type: str, data: dict):
    if event_type == "job_started":
        print(f"Job {data['job_id']} started")
    elif event_type == "job_completed":
        print(f"Job {data['job_id']} completed")

orchestrator.add_status_callback(status_callback)
```

### Event Types

- `job_started` - Job execution begins
- `job_completed` - Job completes successfully
- `job_failed` - Job fails after all retries
- `job_cancelled` - Job cancelled by user
- `job_retrying` - Job failed, retrying
- `job_skipped` - Job skipped due to dependency failure

## Advanced Features

### Conditional Execution

Jobs can be skipped based on conditions:

```yaml
conditions:
  skip_if_failed: ["build"]  # Skip if build job failed
  only_if_branch: "main"     # Only run on main branch
```

### Job Tags and Filtering

```yaml
tags: ["build", "frontend", "critical"]
```

Use tags for selective execution or monitoring.

### DAG Visualization

Generate execution plans and critical path analysis:

```bash
python src/main.py analyze workflow.yaml --output analysis.json
```

### Custom Job Types

Extend the system with custom job types:

```python
from executor import JobExecutor

class CustomJobExecutor(JobExecutor):
    async def _execute_custom(self, job_execution):
        # Custom execution logic
        pass
```

## Error Handling

### Common Issues

1. **Circular Dependencies**
   ```
   Error: Circular dependency detected: job1 -> job2 -> job1
   ```
   Solution: Review dependency chain and remove cycles

2. **Missing Dependencies**
   ```
   Error: Job 'deploy' depends on non-existent job 'missing'
   ```
   Solution: Ensure all referenced job IDs exist

3. **Resource Exhaustion**
   ```
   Error: Insufficient resources to start job
   ```
   Solution: Increase limits or reduce concurrent jobs

4. **Job Timeouts**
   ```
   Error: Job timed out after 300 seconds
   ```
   Solution: Increase timeout or optimize job

### Debugging

Enable debug logging:

```bash
export LOG_LEVEL=DEBUG
python src/main.py run workflow.yaml
```

View detailed execution history:

```bash
python src/main.py run workflow.yaml --output results.json
cat results.json | jq '.jobs[] | select(.status == "failed")'
```

## Performance Tuning

### Concurrency Settings

```yaml
# Optimize for CPU-bound jobs
max_concurrent_jobs: $(nproc)

# Optimize for I/O-bound jobs  
max_concurrent_jobs: $(($(nproc) * 2))
```

### Resource Allocation

```yaml
resource_limits:
  max_memory_mb: 512      # Reduce for many small jobs
  max_execution_time: 300  # Set realistic timeouts
```

### DAG Optimization

- **Minimize critical path**: Parallelize independent operations
- **Balance levels**: Avoid too many jobs in single level
- **Resource awareness**: Consider memory/CPU requirements

## Testing

Run the included test suite:

```bash
# Unit tests
python -m pytest tests/unit/

# Integration tests
python -m pytest tests/integration/

# Full test suite
python -m pytest tests/ -v --cov=src
```

### Test Workflows

```bash
# Test with sample workflow
python src/main.py run config/sample_workflow.yaml

# Create test workflow
python src/main.py create --name "Test" --output test.yaml
python src/main.py run test.yaml --dry-run
```

## Production Deployment

### Kubernetes Deployment

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: job-orchestrator
spec:
  replicas: 1
  selector:
    matchLabels:
      app: job-orchestrator
  template:
    metadata:
      labels:
        app: job-orchestrator
    spec:
      containers:
      - name: orchestrator
        image: job-orchestrator:latest
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "2"
        volumeMounts:
        - name: workflows
          mountPath: /app/workflows
      volumes:
      - name: workflows
        configMap:
          name: workflow-config
```

### Security Considerations

- **Sandboxing**: Run jobs in isolated containers
- **Resource limits**: Prevent resource exhaustion attacks
- **Input validation**: Sanitize workflow definitions
- **Access control**: Restrict workflow modification capabilities

## License

This implementation is part of the Req2Run benchmark suite.

---

<a id="japanese"></a>
## æ—¥æœ¬èª

DAGï¼ˆæœ‰å‘éå·¡å›ã‚°ãƒ©ãƒ•ï¼‰ä¾å­˜é–¢ä¿‚ã‚’æŒã¤ä¸¦åˆ—ã‚¸ãƒ§ãƒ–å®Ÿè¡Œã‚’ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ãƒˆã™ã‚‹ãŸã‚ã®æ´—ç·´ã•ã‚ŒãŸã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ãƒ„ãƒ¼ãƒ«ã€‚ãƒªã‚½ãƒ¼ã‚¹ç®¡ç†ã€ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–æ©Ÿèƒ½ã‚’å‚™ãˆã¦ã„ã¾ã™ã€‚

## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¦‚è¦

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ« â”‚â”€â”€â”€â–¶â”‚   DAGã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼   â”‚â”€â”€â”€â–¶â”‚  ã‚¸ãƒ§ãƒ–ã‚¨ã‚°ã‚¼ã‚­ãƒ¥ãƒ¼ã‚¿  â”‚
â”‚  (YAML/JSON)    â”‚    â”‚                  â”‚    â”‚                 â”‚
â”‚                 â”‚    â”‚ â€¢ æ¤œè¨¼           â”‚    â”‚ â€¢ ãƒªã‚½ãƒ¼ã‚¹ç®¡ç†    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ â€¢ ã‚µã‚¤ã‚¯ãƒ«æ¤œå‡º     â”‚    â”‚ â€¢ ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯ â”‚
                       â”‚ â€¢ ãƒ¬ãƒ™ãƒ«è¨ˆç”»      â”‚    â”‚ â€¢ ãƒ—ãƒ­ã‚»ã‚¹ç®¡ç†    â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚                         â”‚
                               â–¼                         â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚  ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼  â”‚â—„â”€â”€â”€â”‚  ãƒ©ã‚¤ãƒ–ãƒ¢ãƒ‹ã‚¿ãƒ¼    â”‚
                       â”‚                  â”‚    â”‚                 â”‚
                       â”‚ â€¢ ä¸¦åˆ—å®Ÿè¡Œ        â”‚    â”‚ â€¢ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ UI  â”‚
                       â”‚ â€¢ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¿½è·¡   â”‚    â”‚ â€¢ é€²è¡ŒçŠ¶æ³ãƒãƒ¼    â”‚
                       â”‚ â€¢ ã‚¤ãƒ™ãƒ³ãƒˆå‡¦ç†     â”‚    â”‚ â€¢ ãƒªã‚½ãƒ¼ã‚¹çµ±è¨ˆ    â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ä¸»è¦æ©Ÿèƒ½

### ğŸ—ï¸ DAGãƒ™ãƒ¼ã‚¹å®Ÿè¡Œ
- **ä¾å­˜é–¢ä¿‚è§£æ±º**: ã‚¸ãƒ§ãƒ–ä¾å­˜é–¢ä¿‚ã®è‡ªå‹•ãƒˆãƒãƒ­ã‚¸ã‚«ãƒ«ã‚½ãƒ¼ãƒˆ
- **ã‚µã‚¤ã‚¯ãƒ«æ¤œå‡º**: æ˜ç¢ºãªã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã§å¾ªç’°ä¾å­˜ã‚’é˜²æ­¢
- **ä¸¦åˆ—å®Ÿè¡Œ**: ãƒªã‚½ãƒ¼ã‚¹é™ç•Œã¾ã§ç‹¬ç«‹ã‚¸ãƒ§ãƒ–ã‚’åŒæ™‚å®Ÿè¡Œ
- **ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãƒ‘ã‚¹åˆ†æ**: æœ€é©åŒ–ã®ãŸã‚ã®æœ€é•·å®Ÿè¡Œãƒ‘ã‚¹ã‚’ç‰¹å®š

### ğŸ”„ é«˜åº¦ãªã‚¸ãƒ§ãƒ–ç®¡ç†
- **è¤‡æ•°ã®ã‚¸ãƒ§ãƒ–ã‚¿ã‚¤ãƒ—**: ã‚³ãƒãƒ³ãƒ‰ã€ã‚¹ã‚¯ãƒªãƒ—ãƒˆã€Pythonã‚³ãƒ¼ãƒ‰ã€HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆ
- **ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯**: è¨­å®šå¯èƒ½ãªæœ€å¤§è©¦è¡Œå›æ•°ã§ã®æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
- **æ¡ä»¶ä»˜ãå®Ÿè¡Œ**: ä¾å­˜é–¢ä¿‚ã®å¤±æ•—ã«åŸºã¥ã„ã¦ã‚¸ãƒ§ãƒ–ã‚’ã‚¹ã‚­ãƒƒãƒ—
- **ãƒªã‚½ãƒ¼ã‚¹åˆ¶é™**: CPUã€ãƒ¡ãƒ¢ãƒªã€å®Ÿè¡Œæ™‚é–“ã®åˆ¶ç´„

### ğŸ“Š ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–
- **ãƒ©ã‚¤ãƒ–é€²è¡ŒçŠ¶æ³è¡¨ç¤º**: é€²è¡ŒçŠ¶æ³ãƒãƒ¼ã¨ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ä»˜ããƒªãƒƒãƒã‚¿ãƒ¼ãƒŸãƒŠãƒ«UI
- **ãƒªã‚½ãƒ¼ã‚¹è¿½è·¡**: ã™ã¹ã¦ã®å®Ÿè¡Œä¸­ã‚¸ãƒ§ãƒ–ã®CPUã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ç›£è¦–
- **ã‚¤ãƒ™ãƒ³ãƒˆå±¥æ­´**: ã™ã¹ã¦ã®ã‚¸ãƒ§ãƒ–çŠ¶æ…‹å¤‰æ›´ã®å®Œå…¨ãªç›£æŸ»è¨¼è·¡
- **ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯**: ã‚«ã‚¹ã‚¿ãƒ ç›£è¦–ç”¨ã®æ‹¡å¼µå¯èƒ½ãªã‚¤ãƒ™ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ 

### âš™ï¸ ãƒªã‚½ãƒ¼ã‚¹ç®¡ç†
- **åŒæ™‚ã‚¸ãƒ§ãƒ–åˆ¶é™**: è¨­å®šå¯èƒ½ãªæœ€å¤§ä¸¦åˆ—ã‚¸ãƒ§ãƒ–å®Ÿè¡Œ
- **ãƒ¡ãƒ¢ãƒªç®¡ç†**: ã‚¸ãƒ§ãƒ–ã”ã¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®è¿½è·¡ã¨åˆ¶é™
- **ãƒ—ãƒ­ã‚»ã‚¹ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«**: é©åˆ‡ãªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã¨ã‚·ã‚°ãƒŠãƒ«å‡¦ç†
- **ã‚°ãƒ¬ãƒ¼ã‚¹ãƒ•ãƒ«ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³**: å®Ÿè¡Œä¸­ã‚¸ãƒ§ãƒ–ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã§ã‚¯ãƒªãƒ¼ãƒ³çµ‚äº†

## ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### å‰ææ¡ä»¶

- Python 3.11+
- ä¾å­˜é–¢ä¿‚: `pip install -r requirements.txt`

### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# ã‚¯ãƒ­ãƒ¼ãƒ³ã¨ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
cd baselines/CLI-011
pip install -r requirements.txt

# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª
python src/main.py --help
```

### åŸºæœ¬çš„ãªä½¿ç”¨æ³•

```bash
# ã‚µãƒ³ãƒ—ãƒ«ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ä½œæˆ
python src/main.py create --name "My Pipeline" --output workflow.yaml

# ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼æ§‹é€ ã‚’åˆ†æ
python src/main.py analyze workflow.yaml

# ãƒ©ã‚¤ãƒ–ç›£è¦–ã§ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè¡Œ
python src/main.py run workflow.yaml --max-concurrent 4

# å®Ÿè¡Œã›ãšã«æ¤œè¨¼ã™ã‚‹ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³
python src/main.py run workflow.yaml --dry-run
```

### Dockerä½¿ç”¨æ³•

```bash
# ã‚³ãƒ³ãƒ†ãƒŠã‚’ãƒ“ãƒ«ãƒ‰
docker build -t job-orchestrator .

# ãƒã‚¦ãƒ³ãƒˆã•ã‚ŒãŸãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã§å®Ÿè¡Œ
docker run -v $(pwd)/workflows:/app/workflows job-orchestrator run /app/workflows/sample.yaml

# ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰
docker run -it job-orchestrator bash
```

## ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®šç¾©

### YAMLå½¢å¼

```yaml
name: "CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"
version: "1.0.0"
description: "ãƒ“ãƒ«ãƒ‰ã¨ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"

global_config:
  max_retries: 3
  timeout: 3600

jobs:
  - id: "build"
    name: "ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ“ãƒ«ãƒ‰"
    job_type: "command"
    command: "make build"
    dependencies: []
    timeout: 300
    retry_config:
      max_attempts: 3
      initial_delay: 1.0
      backoff_factor: 2.0
    resource_limits:
      max_memory_mb: 1024
      max_execution_time: 600
    environment:
      NODE_ENV: "production"
    tags: ["build", "compile"]

  - id: "test"
    name: "ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"
    job_type: "script"
    script_path: "./scripts/test.sh"
    dependencies: ["build"]
    timeout: 600

  - id: "deploy"
    name: "ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ—ãƒ­ã‚¤"
    job_type: "command"
    command: "kubectl apply -f deployment.yaml"
    dependencies: ["test"]
    timeout: 900
```

### ã‚¸ãƒ§ãƒ–ã‚¿ã‚¤ãƒ—

#### ã‚³ãƒãƒ³ãƒ‰ã‚¸ãƒ§ãƒ–
```yaml
job_type: "command"
command: "echo 'Hello World' && ls -la"
working_directory: "/app"
environment:
  PATH: "/usr/local/bin:$PATH"
```

#### ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚¸ãƒ§ãƒ–
```yaml
job_type: "script"
script_path: "./scripts/deploy.sh"
working_directory: "/app"
```

#### Pythonã‚¸ãƒ§ãƒ–
```yaml
job_type: "python"
command: |
  import os
  print(f"ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {os.getcwd()}")
  # Pythonã‚³ãƒ¼ãƒ‰ã¯ã“ã“ã«
```

#### HTTPã‚¸ãƒ§ãƒ–
```yaml
job_type: "http"
command: "https://api.example.com/health"
timeout: 30
```

## CLIã‚³ãƒãƒ³ãƒ‰

### `run` - ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œ

```bash
python src/main.py run workflow.yaml [OPTIONS]

ã‚ªãƒ—ã‚·ãƒ§ãƒ³:
  -c, --max-concurrent INTEGER  æœ€å¤§åŒæ™‚ã‚¸ãƒ§ãƒ–æ•° [ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 4]
  --dry-run                    å®Ÿè¡Œã›ãšã«ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’æ¤œè¨¼
  -o, --output PATH            çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›
  --live / --no-live          ãƒ©ã‚¤ãƒ–é€²è¡ŒçŠ¶æ³è¡¨ç¤ºã‚’è¡¨ç¤º [ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: True]
```

### `analyze` - DAGæ§‹é€ åˆ†æ

```bash
python src/main.py analyze workflow.yaml [OPTIONS]

ã‚ªãƒ—ã‚·ãƒ§ãƒ³:
  -o, --output PATH  åˆ†æã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›
```

å‡ºåŠ›ä¾‹:
```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£           â”ƒ å€¤                  â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å        â”‚ ã‚µãƒ³ãƒ—ãƒ«CI/CD        â”‚
â”‚ ç·ã‚¸ãƒ§ãƒ–æ•°           â”‚ 14                  â”‚
â”‚ æœ‰åŠ¹ãªDAG           â”‚ âœ“                   â”‚
â”‚ ã‚µã‚¤ã‚¯ãƒ«æœ‰ç„¡         â”‚ âœ“                   â”‚
â”‚ å®Ÿè¡Œãƒ¬ãƒ™ãƒ«           â”‚ 8                   â”‚
â”‚ ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãƒ‘ã‚¹     â”‚ 7                   â”‚
â”‚ æœ€å¤§ä¸¦åˆ—åº¦           â”‚ 3                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å®Ÿè¡Œé †åº:
  ãƒ¬ãƒ™ãƒ« 0: setup
  ãƒ¬ãƒ™ãƒ« 1: install_deps
  ãƒ¬ãƒ™ãƒ« 2: lint, build_frontend, build_backend
  ãƒ¬ãƒ™ãƒ« 3: test_unit_frontend, test_unit_backend, security_scan
  ...
```

### `create` - ã‚µãƒ³ãƒ—ãƒ«ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ç”Ÿæˆ

```bash
python src/main.py create --name "ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å" --output workflow.yaml
```

## è¨­å®š

### ãƒªãƒˆãƒ©ã‚¤è¨­å®š

```yaml
retry_config:
  max_attempts: 3        # æœ€å¤§ãƒªãƒˆãƒ©ã‚¤è©¦è¡Œå›æ•°
  initial_delay: 1.0     # åˆæœŸé…å»¶ï¼ˆç§’ï¼‰
  max_delay: 60.0        # ãƒªãƒˆãƒ©ã‚¤é–“ã®æœ€å¤§é…å»¶
  backoff_factor: 2.0    # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ä¹—æ•°
```

### ãƒªã‚½ãƒ¼ã‚¹åˆ¶é™

```yaml
resource_limits:
  max_memory_mb: 1024         # ã‚¸ãƒ§ãƒ–ã”ã¨ã®æœ€å¤§ãƒ¡ãƒ¢ãƒª
  max_cpu_percent: 100.0      # CPUä½¿ç”¨åˆ¶é™
  max_execution_time: 3600    # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰
  max_concurrent_jobs: 4      # ã‚°ãƒ­ãƒ¼ãƒãƒ«åŒæ™‚å®Ÿè¡Œåˆ¶é™
```

### ç’°å¢ƒå¤‰æ•°

```yaml
environment:
  NODE_ENV: "production"
  API_URL: "https://api.example.com"
  DEBUG: "false"
```

## ç›£è¦–ã¨ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹

### ãƒ©ã‚¤ãƒ–è¡¨ç¤º

ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ã¯ä»¥ä¸‹ã‚’ç¤ºã™ãƒªãƒƒãƒã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’æä¾›:

- **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¸ãƒ§ãƒ–ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹** ã¨é€²è¡ŒçŠ¶æ³ãƒãƒ¼
- **ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡** (CPUã€ãƒ¡ãƒ¢ãƒªã€ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¸ãƒ§ãƒ–)
- **å®Ÿè¡Œã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³** é–‹å§‹/å®Œäº†æ™‚åˆ»ä»˜ã
- **ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸** ã¨ãƒªãƒˆãƒ©ã‚¤è©¦è¡Œ

### ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯

```python
def status_callback(event_type: str, data: dict):
    if event_type == "job_started":
        print(f"ã‚¸ãƒ§ãƒ– {data['job_id']} é–‹å§‹")
    elif event_type == "job_completed":
        print(f"ã‚¸ãƒ§ãƒ– {data['job_id']} å®Œäº†")

orchestrator.add_status_callback(status_callback)
```

### ã‚¤ãƒ™ãƒ³ãƒˆã‚¿ã‚¤ãƒ—

- `job_started` - ã‚¸ãƒ§ãƒ–å®Ÿè¡Œé–‹å§‹
- `job_completed` - ã‚¸ãƒ§ãƒ–æ­£å¸¸å®Œäº†
- `job_failed` - ã™ã¹ã¦ã®ãƒªãƒˆãƒ©ã‚¤å¾Œã«ã‚¸ãƒ§ãƒ–å¤±æ•—
- `job_cancelled` - ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚Šã‚¸ãƒ§ãƒ–ã‚­ãƒ£ãƒ³ã‚»ãƒ«
- `job_retrying` - ã‚¸ãƒ§ãƒ–å¤±æ•—ã€ãƒªãƒˆãƒ©ã‚¤ä¸­
- `job_skipped` - ä¾å­˜é–¢ä¿‚ã®å¤±æ•—ã«ã‚ˆã‚Šã‚¸ãƒ§ãƒ–ã‚¹ã‚­ãƒƒãƒ—

## é«˜åº¦ãªæ©Ÿèƒ½

### æ¡ä»¶ä»˜ãå®Ÿè¡Œ

æ¡ä»¶ã«åŸºã¥ã„ã¦ã‚¸ãƒ§ãƒ–ã‚’ã‚¹ã‚­ãƒƒãƒ—å¯èƒ½:

```yaml
conditions:
  skip_if_failed: ["build"]  # ãƒ“ãƒ«ãƒ‰ã‚¸ãƒ§ãƒ–ãŒå¤±æ•—ã—ãŸå ´åˆã‚¹ã‚­ãƒƒãƒ—
  only_if_branch: "main"     # mainãƒ–ãƒ©ãƒ³ãƒã§ã®ã¿å®Ÿè¡Œ
```

### ã‚¸ãƒ§ãƒ–ã‚¿ã‚°ã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°

```yaml
tags: ["build", "frontend", "critical"]
```

é¸æŠçš„å®Ÿè¡Œã¾ãŸã¯ç›£è¦–ã«ã‚¿ã‚°ã‚’ä½¿ç”¨ã€‚

### DAGè¦–è¦šåŒ–

å®Ÿè¡Œè¨ˆç”»ã¨ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãƒ‘ã‚¹åˆ†æã‚’ç”Ÿæˆ:

```bash
python src/main.py analyze workflow.yaml --output analysis.json
```

### ã‚«ã‚¹ã‚¿ãƒ ã‚¸ãƒ§ãƒ–ã‚¿ã‚¤ãƒ—

ã‚«ã‚¹ã‚¿ãƒ ã‚¸ãƒ§ãƒ–ã‚¿ã‚¤ãƒ—ã§ã‚·ã‚¹ãƒ†ãƒ ã‚’æ‹¡å¼µ:

```python
from executor import JobExecutor

class CustomJobExecutor(JobExecutor):
    async def _execute_custom(self, job_execution):
        # ã‚«ã‚¹ã‚¿ãƒ å®Ÿè¡Œãƒ­ã‚¸ãƒƒã‚¯
        pass
```

## ã‚¨ãƒ©ãƒ¼å‡¦ç†

### ä¸€èˆ¬çš„ãªå•é¡Œ

1. **å¾ªç’°ä¾å­˜**
   ```
   ã‚¨ãƒ©ãƒ¼: å¾ªç’°ä¾å­˜ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ: job1 -> job2 -> job1
   ```
   è§£æ±ºç­–: ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒ¼ãƒ³ã‚’ç¢ºèªã—ã€ã‚µã‚¤ã‚¯ãƒ«ã‚’å‰Šé™¤

2. **ä¾å­˜é–¢ä¿‚ã®æ¬ è½**
   ```
   ã‚¨ãƒ©ãƒ¼: ã‚¸ãƒ§ãƒ– 'deploy' ãŒå­˜åœ¨ã—ãªã„ã‚¸ãƒ§ãƒ– 'missing' ã«ä¾å­˜
   ```
   è§£æ±ºç­–: ã™ã¹ã¦ã®å‚ç…§ã•ã‚Œã‚‹ã‚¸ãƒ§ãƒ–IDãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèª

3. **ãƒªã‚½ãƒ¼ã‚¹æ¯æ¸‡**
   ```
   ã‚¨ãƒ©ãƒ¼: ã‚¸ãƒ§ãƒ–ã‚’é–‹å§‹ã™ã‚‹ãŸã‚ã®ä¸ååˆ†ãªãƒªã‚½ãƒ¼ã‚¹
   ```
   è§£æ±ºç­–: åˆ¶é™ã‚’å¢—ã‚„ã™ã‹åŒæ™‚ã‚¸ãƒ§ãƒ–ã‚’å‰Šæ¸›

4. **ã‚¸ãƒ§ãƒ–ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ**
   ```
   ã‚¨ãƒ©ãƒ¼: 300ç§’å¾Œã«ã‚¸ãƒ§ãƒ–ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
   ```
   è§£æ±ºç­–: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’å¢—ã‚„ã™ã‹ã‚¸ãƒ§ãƒ–ã‚’æœ€é©åŒ–

### ãƒ‡ãƒãƒƒã‚°

ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ã‚’æœ‰åŠ¹åŒ–:

```bash
export LOG_LEVEL=DEBUG
python src/main.py run workflow.yaml
```

è©³ç´°ãªå®Ÿè¡Œå±¥æ­´ã‚’è¡¨ç¤º:

```bash
python src/main.py run workflow.yaml --output results.json
cat results.json | jq '.jobs[] | select(.status == "failed")'
```

## ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

### åŒæ™‚å®Ÿè¡Œè¨­å®š

```yaml
# CPUãƒã‚¦ãƒ³ãƒ‰ã‚¸ãƒ§ãƒ–ç”¨ã«æœ€é©åŒ–
max_concurrent_jobs: $(nproc)

# I/Oãƒã‚¦ãƒ³ãƒ‰ã‚¸ãƒ§ãƒ–ç”¨ã«æœ€é©åŒ–
max_concurrent_jobs: $(($(nproc) * 2))
```

### ãƒªã‚½ãƒ¼ã‚¹å‰²ã‚Šå½“ã¦

```yaml
resource_limits:
  max_memory_mb: 512      # å¤šãã®å°ã•ãªã‚¸ãƒ§ãƒ–ç”¨ã«å‰Šæ¸›
  max_execution_time: 300  # ç¾å®Ÿçš„ãªã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’è¨­å®š
```

### DAGæœ€é©åŒ–

- **ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãƒ‘ã‚¹ã®æœ€å°åŒ–**: ç‹¬ç«‹ã—ãŸæ“ä½œã‚’ä¸¦åˆ—åŒ–
- **ãƒ¬ãƒ™ãƒ«ã®ãƒãƒ©ãƒ³ã‚¹**: å˜ä¸€ãƒ¬ãƒ™ãƒ«ã«å¤šã™ãã‚‹ã‚¸ãƒ§ãƒ–ã‚’é¿ã‘ã‚‹
- **ãƒªã‚½ãƒ¼ã‚¹èªè­˜**: ãƒ¡ãƒ¢ãƒª/CPUè¦ä»¶ã‚’è€ƒæ…®

## ãƒ†ã‚¹ãƒˆ

å«ã¾ã‚Œã‚‹ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã‚’å®Ÿè¡Œ:

```bash
# ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ
python -m pytest tests/unit/

# çµ±åˆãƒ†ã‚¹ãƒˆ
python -m pytest tests/integration/

# å®Œå…¨ãªãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ
python -m pytest tests/ -v --cov=src
```

### ãƒ†ã‚¹ãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼

```bash
# ã‚µãƒ³ãƒ—ãƒ«ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã§ãƒ†ã‚¹ãƒˆ
python src/main.py run config/sample_workflow.yaml

# ãƒ†ã‚¹ãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ä½œæˆ
python src/main.py create --name "Test" --output test.yaml
python src/main.py run test.yaml --dry-run
```

## æœ¬ç•ªãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ

### Kubernetesãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: job-orchestrator
spec:
  replicas: 1
  selector:
    matchLabels:
      app: job-orchestrator
  template:
    metadata:
      labels:
        app: job-orchestrator
    spec:
      containers:
      - name: orchestrator
        image: job-orchestrator:latest
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "2"
        volumeMounts:
        - name: workflows
          mountPath: /app/workflows
      volumes:
      - name: workflows
        configMap:
          name: workflow-config
```

### ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è€ƒæ…®äº‹é …

- **ã‚µãƒ³ãƒ‰ãƒœãƒƒã‚¯ã‚¹åŒ–**: åˆ†é›¢ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒŠã§ã‚¸ãƒ§ãƒ–ã‚’å®Ÿè¡Œ
- **ãƒªã‚½ãƒ¼ã‚¹åˆ¶é™**: ãƒªã‚½ãƒ¼ã‚¹æ¯æ¸‡æ”»æ’ƒã‚’é˜²æ­¢
- **å…¥åŠ›æ¤œè¨¼**: ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®šç¾©ã‚’ã‚µãƒ‹ã‚¿ã‚¤ã‚º
- **ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡**: ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å¤‰æ›´æ©Ÿèƒ½ã‚’åˆ¶é™

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

ã“ã®å®Ÿè£…ã¯Req2Runãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¤ãƒ¼ãƒˆã®ä¸€éƒ¨ã§ã™ã€‚