# DATA-010: Stream Processing with Windowing and State

[English](#english) | [æ—¥æœ¬èª](#japanese)

---

<a id="english"></a>
## English

A production-grade stream processing system implementing windowing operations, state management, watermark tracking, and exactly-once processing semantics.

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Event Sources â”‚â”€â”€â”€â–¶â”‚ Stream Processor â”‚â”€â”€â”€â–¶â”‚  Event Sinks    â”‚
â”‚                 â”‚    â”‚                  â”‚    â”‚                 â”‚
â”‚ â€¢ Mock Events   â”‚    â”‚ â€¢ Windowing      â”‚    â”‚ â€¢ Console       â”‚
â”‚ â€¢ File Sources  â”‚    â”‚ â€¢ State Store    â”‚    â”‚ â€¢ File Output   â”‚
â”‚ â€¢ Kafka (TODO)  â”‚    â”‚ â€¢ Watermarks     â”‚    â”‚ â€¢ Metrics       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ â€¢ Pipelines      â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   API Server     â”‚
                       â”‚ â€¢ REST API       â”‚
                       â”‚ â€¢ Metrics        â”‚
                       â”‚ â€¢ Monitoring     â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Key Features

### ğŸ”„ Stream Processing
- **Exactly-once processing semantics** with offset tracking
- **Backpressure handling** with configurable queue sizes
- **Pipeline-based architecture** for multiple processing workflows
- **Asynchronous processing** with configurable parallelism

### ğŸªŸ Windowing Operations
- **Tumbling Windows**: Fixed-size, non-overlapping time windows
- **Sliding Windows**: Fixed-size windows with configurable slide intervals
- **Session Windows**: Dynamic windows based on activity gaps
- **Late data handling** with configurable allowed lateness

### ğŸ’¾ State Management
- **Persistent state storage** using RocksDB (with DiskCache fallback)
- **Checkpointing** for fault tolerance and recovery
- **State scanning** with prefix-based queries
- **Automatic cleanup** of expired state

### ğŸŒŠ Watermark Tracking
- **Event-time processing** with watermark-based progress tracking
- **Per-partition watermarks** for parallel processing
- **Late event detection** and handling
- **Configurable watermark delays** for late data tolerance

### ğŸ“Š Monitoring & Metrics
- **Prometheus metrics** (when available) with comprehensive counters and gauges
- **REST API** for real-time monitoring and control
- **Performance tracking** with latency and throughput metrics
- **Pipeline statistics** and health checks

## Quick Start

### Prerequisites

- Python 3.11+
- Docker (optional)

### Installation

```bash
# Install dependencies
pip install -r requirements.txt

# Create configuration directory
mkdir -p config

# Copy default configuration
cp config/stream.yaml.example config/stream.yaml
```

### Running the Application

```bash
# Start with default configuration
python src/main.py

# Start with custom configuration
python src/main.py --config config/custom.yaml

# Run in Docker
docker build -t data-010-stream-processor .
docker run -p 8080:8080 -p 9090:9090 data-010-stream-processor
```

### Testing the System

```bash
# Check system health
curl http://localhost:8080/health

# View current status
curl http://localhost:8080/status

# List active pipelines
curl http://localhost:8080/pipelines

# Monitor metrics (Prometheus format)
curl http://localhost:9090/metrics
```

## Configuration

### Basic Configuration

```yaml
# config/stream.yaml
app_name: "My Stream Processor"
debug: false

processing:
  parallelism: 4
  queue_size: 10000
  checkpoint_interval: 30.0

windowing:
  windows:
    - type: "tumbling"
      size_ms: 60000  # 1 minute
      allowed_lateness_ms: 5000
```

### Window Types

#### Tumbling Windows
```yaml
- type: "tumbling"
  size_ms: 60000        # Window size: 1 minute
  allowed_lateness_ms: 5000  # Late data tolerance: 5 seconds
```

#### Sliding Windows
```yaml
- type: "sliding"
  size_ms: 300000       # Window size: 5 minutes
  slide_ms: 30000       # Slide interval: 30 seconds
  allowed_lateness_ms: 10000
```

#### Session Windows
```yaml
- type: "session"
  gap_ms: 30000         # Inactivity gap: 30 seconds
  allowed_lateness_ms: 15000
```

## API Reference

### Health & Status

- `GET /health` - System health check
- `GET /status` - Detailed system status and statistics

### Pipeline Management

- `GET /pipelines` - List all pipelines
- `POST /pipelines/{id}/start` - Start a specific pipeline
- `POST /pipelines/{id}/stop` - Stop a specific pipeline
- `GET /pipelines/{id}/stats` - Get pipeline statistics

### Monitoring

- `GET /watermarks` - Current watermark information
- `GET /state` - State store information
- `GET /windows` - Active windows information
- `POST /checkpoint` - Create a system checkpoint

### Metrics (Prometheus)

Available at `http://localhost:9090/metrics`:

- `stream_events_processed_total` - Total events processed
- `stream_processing_duration_seconds` - Processing latency histogram
- `stream_active_windows` - Current active window count
- `stream_watermark_lag_seconds` - Watermark lag by partition

## Processing Model

### Event Flow

1. **Ingestion**: Events are read from sources (mock, file, Kafka)
2. **Routing**: Events are routed to appropriate pipelines
3. **Windowing**: Events are assigned to time-based windows
4. **Processing**: Window functions aggregate or transform data
5. **Output**: Results are written to configured sinks

### Exactly-Once Processing

The system implements exactly-once semantics through:

- **Offset tracking**: Each event has a unique source/partition/offset
- **Deduplication**: Processed events are tracked to prevent reprocessing
- **Checkpointing**: State is periodically saved for recovery
- **Atomic commits**: State updates and output are coordinated

### Fault Tolerance

- **Graceful shutdown**: Clean resource cleanup on termination
- **Error handling**: Comprehensive error handling with retries
- **State recovery**: Restore from checkpoints after failures
- **Circuit breaking**: Prevent cascade failures

## Performance Tuning

### Configuration Parameters

```yaml
processing:
  parallelism: 8          # Increase for higher throughput
  queue_size: 50000       # Larger queues for burst handling
  checkpoint_interval: 60 # Less frequent checkpoints for performance

watermarks:
  delay_seconds: 2.0      # Reduce for lower latency
```

### Resource Requirements

- **CPU**: 1-4 cores depending on parallelism
- **Memory**: 2-8GB depending on state size and window configuration
- **Storage**: SSD recommended for state storage
- **Network**: 1Gbps for high-throughput scenarios

## Development

### Project Structure

```
src/
â”œâ”€â”€ main.py              # Application entry point
â”œâ”€â”€ stream_processor.py  # Core processing engine
â”œâ”€â”€ models.py           # Data models and types
â”œâ”€â”€ windowing.py        # Window implementations
â”œâ”€â”€ state_store.py      # Persistent state management
â”œâ”€â”€ watermark_tracker.py # Watermark tracking
â”œâ”€â”€ sources.py          # Event sources
â”œâ”€â”€ sinks.py            # Event sinks
â”œâ”€â”€ api_server.py       # REST API server
â”œâ”€â”€ metrics.py          # Metrics collection
â””â”€â”€ config.py           # Configuration management
```

### Adding Custom Sources

```python
from src.sources import EventSource
from src.models import StreamEvent

class CustomSource(EventSource):
    async def read_batch(self, max_events: int = 100) -> List[StreamEvent]:
        # Implement custom event reading logic
        pass
    
    async def close(self):
        # Cleanup resources
        pass
```

### Adding Custom Sinks

```python
from src.sinks import EventSink
from src.models import ProcessingResult

class CustomSink(EventSink):
    async def write(self, result: ProcessingResult):
        # Implement custom output logic
        pass
    
    async def close(self):
        # Cleanup resources
        pass
```

## Testing

```bash
# Run unit tests
python -m pytest tests/

# Run integration tests
python -m pytest tests/integration/

# Run performance tests
python tests/performance/throughput_test.py
```

## Production Deployment

### Docker Deployment

```dockerfile
FROM python:3.11-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY src/ src/
COPY config/ config/

EXPOSE 8080 9090

CMD ["python", "src/main.py"]
```

### Kubernetes Deployment

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: stream-processor
spec:
  replicas: 3
  selector:
    matchLabels:
      app: stream-processor
  template:
    metadata:
      labels:
        app: stream-processor
    spec:
      containers:
      - name: stream-processor
        image: data-010-stream-processor:latest
        ports:
        - containerPort: 8080
        - containerPort: 9090
        resources:
          requests:
            memory: "2Gi"
            cpu: "1"
          limits:
            memory: "4Gi"
            cpu: "2"
```

## License

This implementation is part of the Req2Run benchmark suite.

---

<a id="japanese"></a>
## æ—¥æœ¬èª

# DATA-010: ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚¤ãƒ³ã‚°ã¨çŠ¶æ…‹ç®¡ç†ã‚’å‚™ãˆãŸã‚¹ãƒˆãƒªãƒ¼ãƒ å‡¦ç†

ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å‡¦ç†ã€çŠ¶æ…‹ç®¡ç†ã€ã‚¦ã‚©ãƒ¼ã‚¿ãƒ¼ãƒãƒ¼ã‚¯è¿½è·¡ã€ãŠã‚ˆã³å³å¯†ãªä¸€åº¦é™ã‚Šã®å‡¦ç†ã‚»ãƒãƒ³ãƒ†ã‚£ã‚¯ã‚¹ã‚’å®Ÿè£…ã—ãŸæœ¬ç•ªç’°å¢ƒå‘ã‘ã‚¹ãƒˆãƒªãƒ¼ãƒ å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ã€‚

## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¦‚è¦

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ã‚¤ãƒ™ãƒ³ãƒˆã‚½ãƒ¼ã‚¹   â”‚â”€â”€â”€â–¶â”‚ ã‚¹ãƒˆãƒªãƒ¼ãƒ ãƒ—ãƒ­ã‚»ãƒƒã‚µ â”‚â”€â”€â”€â–¶â”‚  ã‚¤ãƒ™ãƒ³ãƒˆã‚·ãƒ³ã‚¯   â”‚
â”‚                 â”‚    â”‚                  â”‚    â”‚                 â”‚
â”‚ â€¢ ãƒ¢ãƒƒã‚¯ã‚¤ãƒ™ãƒ³ãƒˆ  â”‚    â”‚ â€¢ ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å‡¦ç†   â”‚    â”‚ â€¢ ã‚³ãƒ³ã‚½ãƒ¼ãƒ«     â”‚
â”‚ â€¢ ãƒ•ã‚¡ã‚¤ãƒ«ã‚½ãƒ¼ã‚¹  â”‚    â”‚ â€¢ çŠ¶æ…‹ã‚¹ãƒˆã‚¢      â”‚    â”‚ â€¢ ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›    â”‚
â”‚ â€¢ Kafka (TODO)  â”‚    â”‚ â€¢ ã‚¦ã‚©ãƒ¼ã‚¿ãƒ¼ãƒãƒ¼ã‚¯  â”‚    â”‚ â€¢ ãƒ¡ãƒˆãƒªã‚¯ã‚¹     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ â€¢ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³     â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   APIã‚µãƒ¼ãƒãƒ¼     â”‚
                       â”‚ â€¢ REST API       â”‚
                       â”‚ â€¢ ãƒ¡ãƒˆãƒªã‚¯ã‚¹      â”‚
                       â”‚ â€¢ ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°     â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ä¸»è¦æ©Ÿèƒ½

### ğŸ”„ ã‚¹ãƒˆãƒªãƒ¼ãƒ å‡¦ç†
- ã‚ªãƒ•ã‚»ãƒƒãƒˆè¿½è·¡ã«ã‚ˆã‚‹**å³å¯†ãªä¸€åº¦é™ã‚Šã®å‡¦ç†ã‚»ãƒãƒ³ãƒ†ã‚£ã‚¯ã‚¹**
- è¨­å®šå¯èƒ½ãªã‚­ãƒ¥ãƒ¼ã‚µã‚¤ã‚ºã«ã‚ˆã‚‹**ãƒãƒƒã‚¯ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼å‡¦ç†**
- è¤‡æ•°ã®å‡¦ç†ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®ãŸã‚ã®**ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ™ãƒ¼ã‚¹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**
- è¨­å®šå¯èƒ½ãªä¸¦åˆ—æ€§ã«ã‚ˆã‚‹**éåŒæœŸå‡¦ç†**

### ğŸªŸ ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å‡¦ç†
- **ã‚¿ãƒ³ãƒ–ãƒªãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦**: å›ºå®šã‚µã‚¤ã‚ºã€éé‡è¤‡ã®æ™‚é–“ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
- **ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦**: è¨­å®šå¯èƒ½ãªã‚¹ãƒ©ã‚¤ãƒ‰é–“éš”ã‚’æŒã¤å›ºå®šã‚µã‚¤ã‚ºã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
- **ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦**: ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ã‚®ãƒ£ãƒƒãƒ—ã«åŸºã¥ãå‹•çš„ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
- è¨­å®šå¯èƒ½ãªè¨±å®¹é…å»¶ã«ã‚ˆã‚‹**é…å»¶ãƒ‡ãƒ¼ã‚¿å‡¦ç†**

### ğŸ’¾ çŠ¶æ…‹ç®¡ç†
- RocksDBï¼ˆDiskCacheãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ãï¼‰ã‚’ä½¿ç”¨ã—ãŸ**æ°¸ç¶šçš„ãªçŠ¶æ…‹ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸**
- ãƒ•ã‚©ãƒ¼ãƒ«ãƒˆãƒˆãƒ¬ãƒ©ãƒ³ã‚¹ã¨ãƒªã‚«ãƒãƒªã®ãŸã‚ã®**ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ**
- ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹ã®ã‚¯ã‚¨ãƒªã«ã‚ˆã‚‹**çŠ¶æ…‹ã‚¹ã‚­ãƒ£ãƒ³**
- æœŸé™åˆ‡ã‚ŒçŠ¶æ…‹ã®**è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—**

### ğŸŒŠ ã‚¦ã‚©ãƒ¼ã‚¿ãƒ¼ãƒãƒ¼ã‚¯è¿½è·¡
- ã‚¦ã‚©ãƒ¼ã‚¿ãƒ¼ãƒãƒ¼ã‚¯ãƒ™ãƒ¼ã‚¹ã®é€²æ—è¿½è·¡ã«ã‚ˆã‚‹**ã‚¤ãƒ™ãƒ³ãƒˆæ™‚é–“å‡¦ç†**
- ä¸¦åˆ—å‡¦ç†ã®ãŸã‚ã®**ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã”ã¨ã®ã‚¦ã‚©ãƒ¼ã‚¿ãƒ¼ãƒãƒ¼ã‚¯**
- **é…å»¶ã‚¤ãƒ™ãƒ³ãƒˆã®æ¤œå‡º**ã¨å‡¦ç†
- é…å»¶ãƒ‡ãƒ¼ã‚¿è¨±å®¹ã®ãŸã‚ã®**è¨­å®šå¯èƒ½ãªã‚¦ã‚©ãƒ¼ã‚¿ãƒ¼ãƒãƒ¼ã‚¯é…å»¶**

### ğŸ“Š ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ï¼†ãƒ¡ãƒˆãƒªã‚¯ã‚¹
- åŒ…æ‹¬çš„ãªã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã¨ã‚²ãƒ¼ã‚¸ã‚’å‚™ãˆãŸ**Prometheusãƒ¡ãƒˆãƒªã‚¯ã‚¹**ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¨åˆ¶å¾¡ã®ãŸã‚ã®**REST API**
- ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã¨ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«ã‚ˆã‚‹**ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¿½è·¡**
- **ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±è¨ˆ**ã¨ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯

## ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### å‰ææ¡ä»¶

- Python 3.11+
- Dockerï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -r requirements.txt

# è¨­å®šãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
mkdir -p config

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã®ã‚³ãƒ”ãƒ¼
cp config/stream.yaml.example config/stream.yaml
```

### ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®å®Ÿè¡Œ

```bash
# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã§é–‹å§‹
python src/main.py

# ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã§é–‹å§‹
python src/main.py --config config/custom.yaml

# Dockerã§å®Ÿè¡Œ
docker build -t data-010-stream-processor .
docker run -p 8080:8080 -p 9090:9090 data-010-stream-processor
```

### ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ

```bash
# ã‚·ã‚¹ãƒ†ãƒ ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
curl http://localhost:8080/health

# ç¾åœ¨ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’è¡¨ç¤º
curl http://localhost:8080/status

# ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ãƒªã‚¹ãƒˆ
curl http://localhost:8080/pipelines

# ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ç›£è¦–ï¼ˆPrometheuså½¢å¼ï¼‰
curl http://localhost:9090/metrics
```

## è¨­å®š

### åŸºæœ¬è¨­å®š

```yaml
# config/stream.yaml
app_name: "My Stream Processor"
debug: false

processing:
  parallelism: 4
  queue_size: 10000
  checkpoint_interval: 30.0

windowing:
  windows:
    - type: "tumbling"
      size_ms: 60000  # 1åˆ†
      allowed_lateness_ms: 5000
```

### ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚¿ã‚¤ãƒ—

#### ã‚¿ãƒ³ãƒ–ãƒªãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
```yaml
- type: "tumbling"
  size_ms: 60000        # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º: 1åˆ†
  allowed_lateness_ms: 5000  # é…å»¶ãƒ‡ãƒ¼ã‚¿è¨±å®¹: 5ç§’
```

#### ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
```yaml
- type: "sliding"
  size_ms: 300000       # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º: 5åˆ†
  slide_ms: 30000       # ã‚¹ãƒ©ã‚¤ãƒ‰é–“éš”: 30ç§’
  allowed_lateness_ms: 10000
```

#### ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
```yaml
- type: "session"
  gap_ms: 30000         # éã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚®ãƒ£ãƒƒãƒ—: 30ç§’
  allowed_lateness_ms: 15000
```

## APIãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹

### ãƒ˜ãƒ«ã‚¹ï¼†ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹

- `GET /health` - ã‚·ã‚¹ãƒ†ãƒ ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
- `GET /status` - è©³ç´°ãªã‚·ã‚¹ãƒ†ãƒ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã¨çµ±è¨ˆ

### ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ç®¡ç†

- `GET /pipelines` - ã™ã¹ã¦ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ãƒªã‚¹ãƒˆ
- `POST /pipelines/{id}/start` - ç‰¹å®šã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’é–‹å§‹
- `POST /pipelines/{id}/stop` - ç‰¹å®šã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’åœæ­¢
- `GET /pipelines/{id}/stats` - ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±è¨ˆã‚’å–å¾—

### ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°

- `GET /watermarks` - ç¾åœ¨ã®ã‚¦ã‚©ãƒ¼ã‚¿ãƒ¼ãƒãƒ¼ã‚¯æƒ…å ±
- `GET /state` - çŠ¶æ…‹ã‚¹ãƒˆã‚¢æƒ…å ±
- `GET /windows` - ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æƒ…å ±
- `POST /checkpoint` - ã‚·ã‚¹ãƒ†ãƒ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä½œæˆ

### ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆPrometheusï¼‰

`http://localhost:9090/metrics`ã§åˆ©ç”¨å¯èƒ½:

- `stream_events_processed_total` - å‡¦ç†ã•ã‚ŒãŸã‚¤ãƒ™ãƒ³ãƒˆç·æ•°
- `stream_processing_duration_seconds` - å‡¦ç†ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
- `stream_active_windows` - ç¾åœ¨ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æ•°
- `stream_watermark_lag_seconds` - ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³åˆ¥ã‚¦ã‚©ãƒ¼ã‚¿ãƒ¼ãƒãƒ¼ã‚¯é…å»¶

## å‡¦ç†ãƒ¢ãƒ‡ãƒ«

### ã‚¤ãƒ™ãƒ³ãƒˆãƒ•ãƒ­ãƒ¼

1. **å–ã‚Šè¾¼ã¿**: ã‚¤ãƒ™ãƒ³ãƒˆã¯ã‚½ãƒ¼ã‚¹ï¼ˆãƒ¢ãƒƒã‚¯ã€ãƒ•ã‚¡ã‚¤ãƒ«ã€Kafkaï¼‰ã‹ã‚‰èª­ã¿å–ã‚‰ã‚Œã‚‹
2. **ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°**: ã‚¤ãƒ™ãƒ³ãƒˆã¯é©åˆ‡ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã•ã‚Œã‚‹
3. **ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚¤ãƒ³ã‚°**: ã‚¤ãƒ™ãƒ³ãƒˆã¯æ™‚é–“ãƒ™ãƒ¼ã‚¹ã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã«å‰²ã‚Šå½“ã¦ã‚‰ã‚Œã‚‹
4. **å‡¦ç†**: ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦é–¢æ•°ãŒãƒ‡ãƒ¼ã‚¿ã‚’é›†ç´„ã¾ãŸã¯å¤‰æ›
5. **å‡ºåŠ›**: çµæœã¯è¨­å®šã•ã‚ŒãŸã‚·ãƒ³ã‚¯ã«æ›¸ãè¾¼ã¾ã‚Œã‚‹

### å³å¯†ãªä¸€åº¦é™ã‚Šã®å‡¦ç†

ã‚·ã‚¹ãƒ†ãƒ ã¯ä»¥ä¸‹ã«ã‚ˆã‚Šå³å¯†ãªä¸€åº¦é™ã‚Šã®ã‚»ãƒãƒ³ãƒ†ã‚£ã‚¯ã‚¹ã‚’å®Ÿè£…:

- **ã‚ªãƒ•ã‚»ãƒƒãƒˆè¿½è·¡**: å„ã‚¤ãƒ™ãƒ³ãƒˆã¯ä¸€æ„ã®ã‚½ãƒ¼ã‚¹/ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³/ã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’æŒã¤
- **é‡è¤‡é™¤å»**: å‡¦ç†æ¸ˆã¿ã‚¤ãƒ™ãƒ³ãƒˆã¯å†å‡¦ç†ã‚’é˜²ããŸã‚ã«è¿½è·¡ã•ã‚Œã‚‹
- **ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ**: çŠ¶æ…‹ã¯ãƒªã‚«ãƒãƒªã®ãŸã‚ã«å®šæœŸçš„ã«ä¿å­˜ã•ã‚Œã‚‹
- **ã‚¢ãƒˆãƒŸãƒƒã‚¯ã‚³ãƒŸãƒƒãƒˆ**: çŠ¶æ…‹æ›´æ–°ã¨å‡ºåŠ›ãŒèª¿æ•´ã•ã‚Œã‚‹

### ãƒ•ã‚©ãƒ¼ãƒ«ãƒˆãƒˆãƒ¬ãƒ©ãƒ³ã‚¹

- **ã‚°ãƒ¬ãƒ¼ã‚¹ãƒ•ãƒ«ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³**: çµ‚äº†æ™‚ã®ã‚¯ãƒªãƒ¼ãƒ³ãªãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
- **ã‚¨ãƒ©ãƒ¼å‡¦ç†**: ãƒªãƒˆãƒ©ã‚¤ã‚’å«ã‚€åŒ…æ‹¬çš„ãªã‚¨ãƒ©ãƒ¼å‡¦ç†
- **çŠ¶æ…‹ãƒªã‚«ãƒãƒª**: éšœå®³å¾Œã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ã®å¾©å…ƒ
- **ã‚µãƒ¼ã‚­ãƒƒãƒˆãƒ–ãƒ¬ãƒ¼ã‚«ãƒ¼**: ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰éšœå®³ã®é˜²æ­¢

## ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

### è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

```yaml
processing:
  parallelism: 8          # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Šã®ãŸã‚ã«å¢—åŠ 
  queue_size: 50000       # ãƒãƒ¼ã‚¹ãƒˆå‡¦ç†ç”¨ã®å¤§ããªã‚­ãƒ¥ãƒ¼
  checkpoint_interval: 60 # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ãŸã‚ã«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆé »åº¦ã‚’æ¸›å°‘

watermarks:
  delay_seconds: 2.0      # ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ä½æ¸›ã®ãŸã‚ã«æ¸›å°‘
```

### ãƒªã‚½ãƒ¼ã‚¹è¦ä»¶

- **CPU**: ä¸¦åˆ—æ€§ã«å¿œã˜ã¦1-4ã‚³ã‚¢
- **ãƒ¡ãƒ¢ãƒª**: çŠ¶æ…‹ã‚µã‚¤ã‚ºã¨ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦è¨­å®šã«å¿œã˜ã¦2-8GB
- **ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸**: çŠ¶æ…‹ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã«ã¯SSDæ¨å¥¨
- **ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯**: é«˜ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚·ãƒŠãƒªã‚ªã§ã¯1Gbps

## é–‹ç™º

### ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ 

```
src/
â”œâ”€â”€ main.py              # ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ
â”œâ”€â”€ stream_processor.py  # ã‚³ã‚¢å‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³
â”œâ”€â”€ models.py           # ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«ã¨å‹
â”œâ”€â”€ windowing.py        # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å®Ÿè£…
â”œâ”€â”€ state_store.py      # æ°¸ç¶šçš„ãªçŠ¶æ…‹ç®¡ç†
â”œâ”€â”€ watermark_tracker.py # ã‚¦ã‚©ãƒ¼ã‚¿ãƒ¼ãƒãƒ¼ã‚¯è¿½è·¡
â”œâ”€â”€ sources.py          # ã‚¤ãƒ™ãƒ³ãƒˆã‚½ãƒ¼ã‚¹
â”œâ”€â”€ sinks.py            # ã‚¤ãƒ™ãƒ³ãƒˆã‚·ãƒ³ã‚¯
â”œâ”€â”€ api_server.py       # REST APIã‚µãƒ¼ãƒãƒ¼
â”œâ”€â”€ metrics.py          # ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
â””â”€â”€ config.py           # è¨­å®šç®¡ç†
```

### ã‚«ã‚¹ã‚¿ãƒ ã‚½ãƒ¼ã‚¹ã®è¿½åŠ 

```python
from src.sources import EventSource
from src.models import StreamEvent

class CustomSource(EventSource):
    async def read_batch(self, max_events: int = 100) -> List[StreamEvent]:
        # ã‚«ã‚¹ã‚¿ãƒ ã‚¤ãƒ™ãƒ³ãƒˆèª­ã¿å–ã‚Šãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè£…
        pass
    
    async def close(self):
        # ãƒªã‚½ãƒ¼ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        pass
```

### ã‚«ã‚¹ã‚¿ãƒ ã‚·ãƒ³ã‚¯ã®è¿½åŠ 

```python
from src.sinks import EventSink
from src.models import ProcessingResult

class CustomSink(EventSink):
    async def write(self, result: ProcessingResult):
        # ã‚«ã‚¹ã‚¿ãƒ å‡ºåŠ›ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè£…
        pass
    
    async def close(self):
        # ãƒªã‚½ãƒ¼ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        pass
```

## ãƒ†ã‚¹ãƒˆ

```bash
# ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ
python -m pytest tests/

# çµ±åˆãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ
python -m pytest tests/integration/

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ
python tests/performance/throughput_test.py
```

## æœ¬ç•ªç’°å¢ƒãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ

### Dockerãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ

```dockerfile
FROM python:3.11-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY src/ src/
COPY config/ config/

EXPOSE 8080 9090

CMD ["python", "src/main.py"]
```

### Kubernetesãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: stream-processor
spec:
  replicas: 3
  selector:
    matchLabels:
      app: stream-processor
  template:
    metadata:
      labels:
        app: stream-processor
    spec:
      containers:
      - name: stream-processor
        image: data-010-stream-processor:latest
        ports:
        - containerPort: 8080
        - containerPort: 9090
        resources:
          requests:
            memory: "2Gi"
            cpu: "1"
          limits:
            memory: "4Gi"
            cpu: "2"
```

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

ã“ã®å®Ÿè£…ã¯Req2Runãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¤ãƒ¼ãƒˆã®ä¸€éƒ¨ã§ã™ã€‚